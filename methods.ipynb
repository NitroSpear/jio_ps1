{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import *\n",
    "from surprise import *\n",
    "from surprise.model_selection import *\n",
    "from sklearn.decomposition import NMF\n",
    "from tensorflow import keras\n",
    "from scipy.sparse import csr_matrix\n",
    "from implicit.als import AlternatingLeastSquares \n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from keras import layers, Input, Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_path = \"ml-100k/ml-100k/u.item\"\n",
    "train_path = \"ml-100k/ml-100k/u3.base\"\n",
    "test_path = \"ml-100k/ml-100k/u3.test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = ['user_id', 'movie_id', 'rating', 'timestamp']\n",
    "test_ratings = pd.read_csv(test_path, sep='\\t', names=column_names)\n",
    "train_ratings = pd.read_csv(train_path, sep='\\t', names=column_names)\n",
    "movies = pd.read_csv(movies_path, sep='|', header=None, encoding='latin1')\n",
    "movies = movies[[0,1]]\n",
    "movies.columns = ['movie_id', 'title']\n",
    "print(train_ratings.head())\n",
    "print(test_ratings.head())\n",
    "print(movies.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_item_matrix = train_ratings.pivot_table(index='user_id', columns='movie_id', values='rating')\n",
    "# print(user_item_matrix.head())\n",
    "user_item_matrix_filled = user_item_matrix.fillna(0)\n",
    "user_distances = pairwise_distances(user_item_matrix_filled, metric='cosine')\n",
    "user_similarity = 1-user_distances\n",
    "user_similarity_df = pd.DataFrame(user_similarity, index=user_item_matrix.index, columns=user_item_matrix.index)\n",
    "# print(user_similarity_df.head())\n",
    "def get_top_k_similar_users(user_id, k = 5):\n",
    "    similar_users = user_similarity_df[user_id].sort_values(ascending = False)\n",
    "    similar_users = similar_users.drop(user_id)\n",
    "    top_k_users = similar_users.head(k).index\n",
    "    return top_k_users\n",
    "def predict_ratings(user_id, k = 5):\n",
    "    top_k_users = get_top_k_similar_users(user_id, k)\n",
    "    similar_users_ratings = user_item_matrix.loc[top_k_users]\n",
    "\n",
    "    user_similarity_scores = user_similarity_df.loc[user_id, top_k_users]\n",
    "    weighted_ratings = similar_users_ratings.mul(user_similarity_scores, axis = 0).sum(axis = 0)\n",
    "\n",
    "    sum_of_similarity_scores = user_similarity_scores.sum()\n",
    "    if sum_of_similarity_scores > 0:\n",
    "        predict_ratings = weighted_ratings / sum_of_similarity_scores\n",
    "    else:\n",
    "        predict_ratings = pd.Series(index = weighted_ratings.index)\n",
    "\n",
    "    rated_items = user_item_matrix.loc[user_id].dropna().index\n",
    "    predict_ratings = predict_ratings.drop(rated_items, errors = 'ignore')\n",
    "\n",
    "    predict_ratings = predict_ratings.sort_values(ascending=False)\n",
    "    return predict_ratings\n",
    "def evaluate_stats_approach(test_data, k = 5):\n",
    "    test_data['predicted_rating'] = test_data.apply(lambda row: predict_ratings(row['user_id'], k).get(row['movie_id'], np.nan), axis=1)\n",
    "    test_data = test_data.dropna()\n",
    "    rmse = np.sqrt(mean_squared_error(test_data['rating'], test_data['predicted_rating']))\n",
    "    return rmse\n",
    "rmse_statistical = evaluate_stats_approach(test_ratings)\n",
    "print(f\"RMSE for statistical approach: {rmse_statistical:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the user-item matrix\n",
    "user_item_matrix_centered = user_item_matrix.sub(user_item_matrix.mean(axis=1), axis=0).fillna(0)\n",
    "user_distances = pairwise_distances(user_item_matrix_centered, metric='cosine')\n",
    "user_similarity = 1 - user_distances\n",
    "user_similarity_df = pd.DataFrame(user_similarity, index=user_item_matrix.index, columns=user_item_matrix.index)\n",
    "\n",
    "def get_top_k_similar_users(user_id, k=5):\n",
    "    similar_users = user_similarity_df[user_id].sort_values(ascending=False)\n",
    "    similar_users = similar_users.drop(user_id)\n",
    "    top_k_users = similar_users.head(k).index\n",
    "    return top_k_users\n",
    "\n",
    "def predict_ratings(user_id, k=5):\n",
    "    top_k_users = get_top_k_similar_users(user_id, k)\n",
    "    similar_users_ratings = user_item_matrix.loc[top_k_users]\n",
    "    user_similarity_scores = user_similarity_df.loc[user_id, top_k_users]\n",
    "    weighted_ratings = similar_users_ratings.mul(user_similarity_scores, axis=0).sum(axis=0)\n",
    "    sum_of_similarity_scores = user_similarity_scores.sum()\n",
    "    \n",
    "    if sum_of_similarity_scores > 0:\n",
    "        predict_ratings = weighted_ratings / sum_of_similarity_scores\n",
    "    else:\n",
    "        predict_ratings = pd.Series(index=weighted_ratings.index)\n",
    "    \n",
    "    rated_items = user_item_matrix.loc[user_id].dropna().index\n",
    "    predict_ratings = predict_ratings.drop(rated_items, errors='ignore')\n",
    "    predict_ratings = predict_ratings.sort_values(ascending=False)\n",
    "    return predict_ratings\n",
    "\n",
    "def evaluate_stats_approach(test_data, k=5):\n",
    "    test_data['predicted_rating'] = test_data.apply(lambda row: predict_ratings(row['user_id'], k).get(row['movie_id'], np.nan), axis=1)\n",
    "    test_data = test_data.dropna()\n",
    "    rmse = np.sqrt(mean_squared_error(test_data['rating'], test_data['predicted_rating']))\n",
    "    return rmse\n",
    "\n",
    "rmse_statistical = evaluate_stats_approach(test_ratings)\n",
    "print(f\"RMSE for statistical approach: {rmse_statistical:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your existing functions to calculate user similarity and predict ratings\n",
    "def get_top_k_similar_users(user_id, k=5):\n",
    "    similar_users = user_similarity_df[user_id].sort_values(ascending=False)\n",
    "    similar_users = similar_users.drop(user_id)\n",
    "    top_k_users = similar_users.head(k).index\n",
    "    return top_k_users\n",
    "\n",
    "def predict_ratings(user_id, k=5):\n",
    "    top_k_users = get_top_k_similar_users(user_id, k)\n",
    "    similar_users_ratings = user_item_matrix.loc[top_k_users]\n",
    "    user_similarity_scores = user_similarity_df.loc[user_id, top_k_users]\n",
    "    weighted_ratings = similar_users_ratings.mul(user_similarity_scores, axis=0).sum(axis=0)\n",
    "    sum_of_similarity_scores = user_similarity_scores.sum()\n",
    "    \n",
    "    if sum_of_similarity_scores > 0:\n",
    "        predict_ratings = weighted_ratings / sum_of_similarity_scores\n",
    "    else:\n",
    "        predict_ratings = pd.Series(index=weighted_ratings.index)\n",
    "    \n",
    "    rated_items = user_item_matrix.loc[user_id].dropna().index\n",
    "    predict_ratings = predict_ratings.drop(rated_items, errors='ignore')\n",
    "    predict_ratings = predict_ratings.sort_values(ascending=False)\n",
    "    return predict_ratings\n",
    "\n",
    "# Hierarchical clustering functions\n",
    "def find_closest_clusters(distance_matrix):\n",
    "    # Use np.triu_indices to get upper triangle indices\n",
    "    i, j = np.triu_indices(distance_matrix.shape[0], k=1)\n",
    "    min_index = np.argmin(distance_matrix[i, j])\n",
    "    return i[min_index], j[min_index]\n",
    "\n",
    "def hierarchical_clustering(data, num_clusters, max_iterations=1000):\n",
    "    clusters = {i: [i] for i in range(len(data))}\n",
    "    distance_matrix = squareform(pdist(data, metric='euclidean'))\n",
    "    \n",
    "    iteration = 0\n",
    "    while len(clusters) > num_clusters and iteration < max_iterations:\n",
    "        print(f\"Iteration {iteration}, Number of clusters: {len(clusters)}\")\n",
    "        \n",
    "        i, j = find_closest_clusters(distance_matrix)\n",
    "        \n",
    "        if i not in clusters or j not in clusters:\n",
    "            print(f\"Unexpected state: clusters {i} or {j} not found\")\n",
    "            break\n",
    "        \n",
    "        print(f\"Merging clusters {i} and {j}\")\n",
    "        clusters[i].extend(clusters[j])\n",
    "        del clusters[j]\n",
    "        \n",
    "        for k in list(clusters.keys()):\n",
    "            if k != i:\n",
    "                dist = min(distance_matrix[i, k], distance_matrix[j, k])\n",
    "                distance_matrix[i, k] = dist\n",
    "                distance_matrix[k, i] = dist\n",
    "        \n",
    "        distance_matrix[j, :] = np.inf\n",
    "        distance_matrix[:, j] = np.inf\n",
    "        \n",
    "        iteration += 1\n",
    "    \n",
    "    if iteration == max_iterations:\n",
    "        print(\"Warning: Maximum iterations reached. Clustering may be incomplete.\")\n",
    "    \n",
    "    return clusters\n",
    "\n",
    "import time\n",
    "\n",
    "def predict_ratings_cluster_optimized(user_ids, movie_ids, clusters, user_item_matrix, default_rating=3.0, batch_size=1000):\n",
    "    user_to_cluster = {}\n",
    "    for cluster_id, users in clusters.items():\n",
    "        for user in users:\n",
    "            user_to_cluster[user] = cluster_id\n",
    "\n",
    "    predictions = np.full(len(user_ids), default_rating)\n",
    "    \n",
    "    unique_clusters = set(user_to_cluster.values())\n",
    "    total_clusters = len(unique_clusters)\n",
    "    \n",
    "    for i, cluster_id in enumerate(unique_clusters):\n",
    "        if i % 10 == 0:  # Print progress every 10 clusters\n",
    "            print(f\"Processing cluster {i+1}/{total_clusters}\")\n",
    "        \n",
    "        cluster_mask = np.array([user_to_cluster.get(user, -1) == cluster_id for user in user_ids])\n",
    "        if not np.any(cluster_mask):\n",
    "            continue\n",
    "        \n",
    "        cluster_users = [user for user, c_id in user_to_cluster.items() if c_id == cluster_id]\n",
    "        cluster_users = [u for u in cluster_users if u in user_item_matrix.index]\n",
    "        \n",
    "        if not cluster_users:\n",
    "            continue\n",
    "        \n",
    "        cluster_ratings = user_item_matrix.loc[cluster_users]\n",
    "        mean_ratings = cluster_ratings.mean(axis=0)\n",
    "        \n",
    "        # Process in batches\n",
    "        mask_indices = np.where(cluster_mask)[0]\n",
    "        for start in range(0, len(mask_indices), batch_size):\n",
    "            end = start + batch_size\n",
    "            batch_indices = mask_indices[start:end]\n",
    "            batch_movies = movie_ids[batch_indices]\n",
    "            batch_predictions = mean_ratings.reindex(batch_movies).fillna(default_rating).values\n",
    "            predictions[batch_indices] = batch_predictions\n",
    "\n",
    "    return predictions\n",
    "\n",
    "def evaluate_cluster_approach_optimized(test_data, clusters, user_item_matrix, default_rating=3.0, batch_size=1000):\n",
    "    print(\"Starting evaluation...\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    user_ids = test_data['user_id'].values\n",
    "    movie_ids = test_data['movie_id'].values\n",
    "    \n",
    "    print(\"Predicting ratings...\")\n",
    "    predicted_ratings = predict_ratings_cluster_optimized(user_ids, movie_ids, clusters, user_item_matrix, default_rating, batch_size)\n",
    "    \n",
    "    print(\"Calculating RMSE...\")\n",
    "    test_data['predicted_rating'] = predicted_ratings\n",
    "    rmse = np.sqrt(mean_squared_error(test_data['rating'], test_data['predicted_rating']))\n",
    "    \n",
    "    print(f\"Evaluation completed in {time.time() - start_time:.2f} seconds\")\n",
    "    return rmse\n",
    "\n",
    "# Number of clusters\n",
    "num_clusters = 2\n",
    "\n",
    "# Performing hierarchical clustering on the user-item matrix\n",
    "clusters = hierarchical_clustering(user_item_matrix.values, num_clusters)\n",
    "# Evaluate the hierarchical clustering approach\n",
    "rmse_cluster = evaluate_cluster_approach_optimized(test_ratings, clusters, user_item_matrix)\n",
    "print(f\"RMSE for hierarchical clustering approach: {rmse_cluster:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = Reader(rating_scale=(1,5))\n",
    "train_data = Dataset.load_from_df(train_ratings[['user_id', 'movie_id', 'rating']], reader)\n",
    "trainset = train_data.build_full_trainset()\n",
    "test_data = Dataset.load_from_df(test_ratings[['user_id', 'movie_id', 'rating']], reader)\n",
    "testset = test_data.construct_testset(test_data.raw_ratings)\n",
    "svd = SVD()\n",
    "svd.fit(trainset)\n",
    "\n",
    "user_id = 1\n",
    "\n",
    "predictions = svd.test(testset)\n",
    "predicted_ratings = [pred.est for pred in predictions]\n",
    "true_ratings = [pred.r_ui for pred in predictions]\n",
    "rmse_model_based = np.sqrt(mean_squared_error(true_ratings, predicted_ratings))\n",
    "print(f\"RMSE for model-based approach (SVD): {rmse_model_based:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise.model_selection import GridSearchCV\n",
    "# the parameter grid\n",
    "param_grid = {\n",
    "    'n_factors': [50, 100, 150],\n",
    "    'lr_all': [0.002, 0.005, 0.007, 0.009, 0.011, 0.013, 0.015],\n",
    "    'reg_all': [0.02, 0.04, 0.06, 0.08, 0.10, 0.12]\n",
    "}\n",
    "gs = GridSearchCV(SVD, param_grid, measures=['rmse'], cv=3)\n",
    "gs.fit(train_data)\n",
    "\n",
    "# best parameters\n",
    "best_params = gs.best_params['rmse']\n",
    "print(f\"Best parameters: {best_params}\")\n",
    "\n",
    "best_svd = SVD(**best_params)\n",
    "best_svd.fit(trainset)\n",
    "\n",
    "predictions = best_svd.test(testset)\n",
    "predicted_ratings = [pred.est for pred in predictions]\n",
    "true_ratings = [pred.r_ui for pred in predictions]\n",
    "rmse_optimized = np.sqrt(mean_squared_error(true_ratings, predicted_ratings))\n",
    "print(f\"Optimized RMSE: {rmse_optimized:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_ids = train_ratings['user_id'].unique()\n",
    "movie_ids = train_ratings['movie_id'].unique()\n",
    "\n",
    "user_id_map = {id: i for i, id in enumerate(user_ids)}\n",
    "movie_id_map = {id: i for i, id in enumerate(movie_ids)}\n",
    "\n",
    "# Add a special index for unknown users/movies\n",
    "unknown_user_id = len(user_id_map)\n",
    "unknown_movie_id = len(movie_id_map)\n",
    "\n",
    "# Map IDs, assigning unknown IDs to a special index\n",
    "train_ratings['user_id_mapped'] = train_ratings['user_id'].map(user_id_map)\n",
    "train_ratings['movie_id_mapped'] = train_ratings['movie_id'].map(movie_id_map)\n",
    "test_ratings['user_id_mapped'] = test_ratings['user_id'].map(user_id_map).fillna(unknown_user_id).astype(int)\n",
    "test_ratings['movie_id_mapped'] = test_ratings['movie_id'].map(movie_id_map).fillna(unknown_movie_id).astype(int)\n",
    "\n",
    "# Create Neural Collaborative Filtering model\n",
    "num_users = len(user_ids) + 1  # +1 for unknown users\n",
    "num_movies = len(movie_ids) + 1  # +1 for unknown movies\n",
    "\n",
    "user_input = Input(shape=(1,))\n",
    "user_embedding = layers.Embedding(num_users, 50)(user_input)\n",
    "user_vec = layers.Flatten()(user_embedding)\n",
    "\n",
    "movie_input = Input(shape=(1,))\n",
    "movie_embedding = layers.Embedding(num_movies, 50)(movie_input)\n",
    "movie_vec = layers.Flatten()(movie_embedding)\n",
    "\n",
    "concat = layers.Concatenate()([user_vec, movie_vec])\n",
    "dense = layers.Dense(128, activation='relu')(concat)\n",
    "output = layers.Dense(1)(dense)\n",
    "\n",
    "model = Model(inputs=[user_input, movie_input], outputs=output)\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "model.fit([train_ratings['user_id_mapped'], train_ratings['movie_id_mapped']], train_ratings['rating'], epochs=5, verbose=1)\n",
    "\n",
    "predicted_ratings = model.predict([test_ratings['user_id_mapped'], test_ratings['movie_id_mapped']])\n",
    "rmse_ncf = np.sqrt(mean_squared_error(test_ratings['rating'], predicted_ratings))\n",
    "print(f\"RMSE for NCF approach: {rmse_ncf:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jioenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
